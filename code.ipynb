
# Image Classifier Code
.
import os
import numpy as np
import glob
import shutil
import matplotlib.pyplot as plt
import cv2
import seaborn as sns

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils

def load_dataset(base_dir,labels):    
    "This function loads and preprocesses images."
    print('Loading...')
    
    #directories for train and test images 
    test_dir = os.path.join(base_dir,'Test')
    train_dir = os.path.join(base_dir,'Training')
    
    x_train = []
    y_train = []
    x_valid = []
    y_valid = []
    x_test = []
    y_test = []
    
    no_of_classes = len(labels)
    
    for category in labels: 
        
        #create paths for categories in labels 
        train_path = os.path.join(train_dir,category)  
        test_path = os.path.join(test_dir,category)

        #training images 
        for img in os.listdir(train_path):            
            #cv2 images are BGR images, convert them to RGB images to be compatible with matplotlib.pyplot functions 
            img_array = cv2.cvtColor(cv2.imread(os.path.join(train_path,img)),cv2.COLOR_BGR2RGB)  # convert to array
            
            #fruits-360 images are 100x100 by default, so there is no need to change their sizes 
            #img_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE)) 
            new_array = np.array(img_array)
            x_train.append(new_array)
            y_train.append(labels.index(category))
        
        #split test images into validation and test images by a ratio of 0.8
        test_image_no = len(os.listdir(test_path))
        split_no = int(test_image_no * 0.8)
        
        #validation images 
        for img in os.listdir(test_path)[:split_no]:
            img_array = cv2.cvtColor(cv2.imread(os.path.join(test_path,img)),cv2.COLOR_BGR2RGB)  # convert to array
            #img_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE)) 
            new_array = np.array(img_array)
            x_valid.append(new_array)
            y_valid.append(labels.index(category))

        #test images 
        for img in os.listdir(test_path)[split_no:]:
            img_array = cv2.cvtColor(cv2.imread(os.path.join(test_path,img)),cv2.COLOR_BGR2RGB)  # convert to array
            #img_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE)) 
            new_array = np.array(img_array)
            x_test.append(new_array)
            y_test.append(labels.index(category))
    
    #prepare label vectors/target vectors
    y_train = np_utils.to_categorical(y_train,no_of_classes)
    y_test = np_utils.to_categorical(y_test,no_of_classes)
    y_valid = np_utils.to_categorical(y_valid,no_of_classes)
    
    #convert list objects to numpy arrays 
    x_train = np.array(x_train)
    y_train = np.array(y_train)
    
    x_test = np.array(x_test)
    y_test = np.array(y_test)
    
    x_valid = np.array(x_valid)    
    y_valid = np.array(y_valid)
    
    #re-scale so that all the pixel values lie within 0 to 1
    x_train = x_train.astype('float32')/255
    x_valid = x_valid.astype('float32')/255
    x_test = x_test.astype('float32')/255

    print('Done!')
    return x_train, y_train, x_valid, y_valid, x_test, y_test 

base_dir = '../input/fruits/fruits-360'
labels = ['Apple Granny Smith', 'Apple Red 1', 'Avocado', 'Banana Lady Finger', 'Cocos', 'Corn', 'Eggplant', 'Mandarine', 'Pepper Green', 'Strawberry']

#fruits-360 images are 100x100
IMG_SHAPE = 100

NUM_CATEGORIES = len(labels)
print('Fruit categories:', labels)
print('Fruit category no:', NUM_CATEGORIES)
          
x_train, y_train, x_valid, y_valid, x_test, y_test  = load_dataset(base_dir,labels)    
print('Train image no:', x_train.shape[0])
print('Validation image no:', x_valid.shape[0])
print('Test image no:', x_test.shape[0])

#let us see one of y_train's elements 
#only one element has value 1(corresponding to its label) and others are 0.
print('y_train[0]:',y_train[0])

#display sample images 
plt.imshow(x_train[0])
plt.title('Train image')
plt.savefig('sample_train_image.png')
plt.show()
index = np.argmax(y_train[0])
print('Category:', labels[index])

plt.imshow(x_valid[0])
plt.title('Validation image')
plt.savefig('sample_validation_image.png')
plt.show()
index = np.argmax(y_valid[0])
print('Category:', labels[index])

plt.imshow(x_test[0])
plt.title('Test image')
plt.savefig('sample_test_image.png')
plt.show()
index = np.argmax(y_test[0])
print('Category:', labels[index])

#Build model 1
model1 = Sequential()

model1.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))
model1.add(MaxPooling2D(pool_size=(2, 2)))

model1.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(pool_size=(2, 2)))

model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(pool_size=(2, 2)))

model1.add(Flatten())
model1.add(Dropout(0.2))
model1.add(Dense(512, activation='relu'))

model1.add(Dropout(0.2))
model1.add(Dense(NUM_CATEGORIES, activation='softmax'))

model1.summary()

#Build model 2
model2 = Sequential()

model2.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))
model2.add(MaxPooling2D(pool_size=(2, 2)))

model2.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))

model2.add(Flatten())
model2.add(Dropout(0.2))
model2.add(Dense(64, activation='relu'))

model2.add(Dropout(0.2))
model2.add(Dense(NUM_CATEGORIES, activation='softmax'))

model2.summary()

model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
#Data augmentation on training set
datagen = ImageDataGenerator(featurewise_center=True,
                             featurewise_std_normalization=True,
                             rotation_range=20,
                             width_shift_range=0.2,
                             height_shift_range=0.2,
                             horizontal_flip=True)

epochs1 = 10
epochs2 = 5
batch_size = 32 

# fits the model on batches with real-time data augmentation
history1 = model1.fit(datagen.flow(x_train, y_train, batch_size=batch_size),
                    steps_per_epoch=len(x_train) / batch_size, epochs=epochs1, shuffle = True, 
                    validation_data=(x_valid, y_valid))
                    
history2 = model2.fit(datagen.flow(x_train, y_train, batch_size=batch_size),
                    steps_per_epoch=len(x_train) / batch_size, epochs=epochs2, shuffle = True, 
                    validation_data=(x_valid, y_valid))

# evaluate and print test accuracy
score1 = model1.evaluate(x_test, y_test, verbose=0)
score2 = model2.evaluate(x_test, y_test, verbose=0)
print('\n', 'Test accuracy of model #1:', score1[1])
print('\n', 'Test accuracy of model #2:', score2[1])

def plot_model_performance(history,epochs,model_name):
    
    "This functions plots training and validation graphs" 
    
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(1,epochs + 1)
    
    plt.figure(figsize=(16, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.savefig(model_name + "_performance.png")
    plt.show()

plot_model_performance(history1,epochs1,"model_v1")
plot_model_performance(history2,epochs2,"model_v2")

#confusion matrix for test set
y_pred2=model1.predict(x_test)
y_pred_classes2=np.argmax(y_pred2,axis=1)
y_true2=np.argmax(y_test,axis=1)

#compute confusion matrix
conf_mat2=tf.math.confusion_matrix(y_true2,y_pred_classes2)

#plot the confusion matrix
f,ax=plt.subplots(figsize=(10,10))
sns.heatmap(conf_mat2,annot=True,fmt=".0f")
ax.set_xticklabels(labels, rotation = 'vertical')
ax.set_yticklabels(labels, rotation = 'horizontal')
plt.savefig('model_v1_confusion_matrix.png')
plt.show()

#confusion matrix for test set
y_pred2=model2.predict(x_test)
y_pred_classes2=np.argmax(y_pred2,axis=1)
y_true2=np.argmax(y_test,axis=1)

#compute confusion matrix
conf_mat2=tf.math.confusion_matrix(y_true2,y_pred_classes2)

#plot the confusion matrix
f,ax=plt.subplots(figsize=(10,10))
sns.heatmap(conf_mat2,annot=True,fmt=".0f")
ax.set_xticklabels(labels, rotation = 'vertical')
ax.set_yticklabels(labels, rotation = 'horizontal')
plt.savefig('model_v2_confusion_matrix.png')
plt.show()

def plot_test_images(predictions, x_test, y_test, labels):
        
    # plot a random sample of test images, their predicted labels, and ground truth
    fig = plt.figure(figsize=(16, 9))
    
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=16, replace=False)):
        
        ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])
        ax.imshow(np.squeeze(x_test[index]))
        pred_index = np.argmax(predictions[index])
        true_index = np.argmax(y_test[index])
        if pred_index == true_index:
            color = 'green'
        else:
            color = 'red'
        plt.xlabel("{} {:2.0f}%".format(labels[pred_index],100*np.max(predictions[index]),labels[true_index]),color=color)
   
    plt.savefig("test_images.png")
    plt.show()

predictions = model2.predict(x_test)
plot_test_images(predictions,x_test,y_test,labels)

#pip install h5py

from keras.models import model_from_json

# serialize model to JSON
model_json = model1.to_json()
with open("model_v1.json", "w") as json_file:
    json_file.write(model_json)
    
model_json = model2.to_json()
with open("model_v2.json", "w") as json_file:
    json_file.write(model_json)
    
# serialize weights to HDF5
model1.save_weights("model_v1.h5")
model2.save_weights("model_v2.h5")
print("Saved models to disk")
\end{lstlisting}

\vspace{20pt}

\begin{lstlisting}

# Object Detector Code

import os
import numpy as np
import matplotlib.pyplot as plt 
import glob
import shutil
import imutils
import cv2

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils

#upload model
from google.colab import files

uploaded = files.upload()

from keras.models import load_model
from keras.models import model_from_json
import numpy

# load json and create model
json_file = open('model_v2.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)

# load weights into new model
model.load_weights("model_v2.h5")
print("Loaded model from disk")

# summarize model
model.summary()

# import the necessary packages
from tensorflow.keras.applications.resnet import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications import imagenet_utils
from imutils.object_detection import non_max_suppression

#detection helper functions
def sliding_window(image, step, ws):
    # slide a window across the image
    for y in range(0, image.shape[0] - ws[1], step):
        for x in range(0, image.shape[1] - ws[0], step):
            # yield the current window
            yield (x, y, image[y:y + ws[1], x:x + ws[0]])

def image_pyramid(image, scale=1.5, minSize=(20, 20)):
    # yield the original image
    yield image

    # keep looping over the image pyramid
    while True:
        # compute the dimensions of the next image in the pyramid
        w = int(image.shape[1] / scale)
        image = imutils.resize(image, width=w)

        # if the resized image does not meet the supplied minimum
        # size, then stop constructing the pyramid
        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:
            break

        # yield the next image in the pyramid
        yield image

# initialize variables used for the object detection procedure
WIDTH,HEIGHT = 500,500
PYR_SCALE = 1.5
WIN_STEP = 30
ROI_SIZE = (100,100)
INPUT_SIZE = (100, 100)

from google.colab import files

uploaded = files.upload()

#path = 'multiple_test_image.jpg'
path = 'apple_tree.jpg'
orig = cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)   # convert to array
dim = (WIDTH, HEIGHT)
orig = cv2.resize(orig, dim)
(H, W) = orig.shape[:2]

plt.imshow(orig)
plt.xlabel("Image size: ({},{})".format(orig.shape[0],orig.shape[1]))
plt.savefig("test_image.png")
files.download("test_image.png") 
plt.show()

# initialize the image pyramid
pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)
print(type(pyramid))

# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image
rois = []
locs = []

from google.colab.patches import cv2_imshow

visualize_check = 1
count = 0

# loop over the image pyramid
for image in pyramid:
  
  # determine the scale factor between the *original* image
  # dimensions and the *current* layer of the pyramid
  scale = W / float(image.shape[1])

  # for each layer of the image pyramid, loop over the sliding
  # window locations
  for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):
      # scale the (x, y)-coordinates of the ROI with respect to the
      # *original* image dimensions
      x = int(x * scale)
      y = int(y * scale)
      w = int(ROI_SIZE[0] * scale)
      h = int(ROI_SIZE[1] * scale)

      # take the ROI and pre-process it so we can later classify
      # the region using Keras/TensorFlow
      roi = cv2.resize(roiOrig, INPUT_SIZE)
      #roi = img_to_array(roi)
      #roi = preprocess_input(roi)
      roi = np.array(roi)
      roi = roi.astype('float32')/255

      # update our list of ROIs and associated coordinates
      rois.append(roi)
      locs.append((x, y, x + w, y + h))
      if count == 5:
        visualize_check = -1
      # check to see if we are visualizing each of the sliding
      # windows in the image pyramid
      if visualize_check > 0:
          # clone the original image and then draw a bounding box
          # surrounding the current region
          clone = orig.copy()
          cv2.rectangle(clone, (x, y), (x + w, y + h),
              (0, 255, 0), 2)

          # show the visualization and current ROI
          print("Visualization")            
          clone = cv2.cvtColor(clone,cv2.COLOR_RGB2BGR)   
          cv2_imshow(clone)
          print("ROI")
          roiOrig = cv2.cvtColor(roiOrig,cv2.COLOR_RGB2BGR)
          cv2_imshow(roiOrig)
          #cv2.waitKey(0)
          count = count + 1

# convert the ROIs to a NumPy array
#rois = np.array(rois, dtype="float32")
rois = np.array(rois)

# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took
print("[INFO] classifying ROIs...")
preds = model.predict(rois)

prediction_array = []
labels = ['Apple Granny Smith', 'Apple Red 1', 'Avocado', 'Banana Lady Finger', 'Cocos', 'Corn', 'Eggplant', 'Mandarine', 'Pepper Green', 'Strawberry']

for i in range(preds.shape[0]):
    pred_index = np.argmax(preds[i])
    prediction_label = labels[pred_index]
    probability = np.max(preds[i])
    prediction_array.append([prediction_label,probability])
    #display predictions 
    print([prediction_label,probability])

prediction_array = np.array(prediction_array)
print(prediction_array.shape)
#print(prediction_array[84])

min_conf = 0.99

labels = {}

# loop over the predictions
for i in range(len(prediction_array)):
    # grab the prediction information for the current ROI
    (label, prob) = prediction_array[i]

    # filter out weak detections by ensuring the predicted probability
    # is greater than the minimum probabilitystac
    if eval(prob) >= min_conf:
        # grab the bounding box associated with the prediction and
        # convert the coordinates
        box = locs[i]

        # grab the list of predictions for the label and add the
        # bounding box and probability to the list
        L = labels.get(label, [])
        L.append((box, prob))
        labels[label] = L

print(labels.keys())

from google.colab.patches import cv2_imshow

# loop over the labels for each of detected objects in the image
for label in labels.keys():

  # clone the original image so that we can draw on it
  print("[INFO] showing results for '{}'".format(label))
  clone = orig.copy()

  # loop over all bounding boxes for the current label
  for (box, prob) in labels[label]:
      # draw the bounding box on the image
      (startX, startY, endX, endY) = box
      cv2.rectangle(clone, (startX, startY), (endX, endY),
          (0, 255, 0), 2)

  # show the results *before* applying non-maxima suppression, then
  # clone the image again so we can display the results *after*
  # applying non-maxima suppression
  print('Before')
  clone = cv2.cvtColor(clone,cv2.COLOR_RGB2BGR)
  cv2_imshow(clone)
  clone = orig.copy()

  # extract the bounding boxes and associated prediction
  # probabilities, then apply non-maxima suppression
  boxes = np.array([p[0] for p in labels[label]])
  proba = np.array([p[1] for p in labels[label]])
  boxes = non_max_suppression(boxes, proba)

  # loop over all bounding boxes that were kept after applying
  # non-maxima suppression
  for (startX, startY, endX, endY) in boxes:
      # draw the bounding box and label on the image
      cv2.rectangle(clone, (startX, startY), (endX, endY),
          (0, 255, 0), 2)
      y = startY - 10 if startY - 10 > 10 else startY + 10
      cv2.putText(clone, label, (startX, y),
          cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)

  # show the output after apply non-maxima suppression
  print('After')
  clone = cv2.cvtColor(clone,cv2.COLOR_RGB2BGR)
  cv2_imshow(clone)
  #cv2.waitKey(0)
